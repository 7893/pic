# Lens 核心架构与算法深度解析 (01-ARCHITECTURE)

Lens 并非一个简单的图片展示工具，而是一个构建在 Cloudflare 边缘计算（Edge Computing）栈之上的、全自动化的视觉语义对齐引擎。本系统设计的初衷是解决“海量非结构化视觉数据”与“自然语言模糊检索”之间的鸿沟。为了在资源极度受限（边缘侧内存 128MB、Unsplash API 限流、AI 计费配额）的环境下实现高性能运行，系统采用了高度解耦的架构与自研的线性增长算法。

---

## 1. 宏观系统设计：双管道解耦哲学

在传统的搜索架构中，采集与检索往往紧耦合，这会导致采集压力直接波及搜索响应。Lens 采用了 **双管道解耦架构 (Dual-Pipeline Decoupled Architecture)**，通过 Cloudflare Queues 和 Workflows 实现物理与逻辑上的彻底分离。

### 1.1 搜索管道 (Search Pipeline)：追求亚秒级的语义对齐

搜索管道是面向用户的同步链路。其核心挑战在于如何在毫秒级时间内，将人类模糊的、带有情绪的查询词（如“治愈系的落日”）转化为高维空间的坐标，并从数万张图片中捞出最匹配的那一张。

- **多层级缓存体系**：为了对抗 AI 推理的高延迟，系统构建了“Edge Cache + KV Semantic Cache”的双重防线。Edge Cache 负责拦截完全一致的请求，而 KV 语义缓存则存储了 Llama 4 对查询词的深度扩展结果。这意味着对于热门搜索，系统几乎无需调用大模型即可完成精准召回。
- **三级漏斗过滤逻辑**：
  1.  **查询扩展 (Query Expansion)**：利用 Llama 3.2 3B 将简短的关键词扩充为包含光影、氛围、构图的视觉描述词。
  2.  **向量初筛 (Retrieval)**：通过 BGE-M3 模型生成的 1024 维密集向量，在 Vectorize 中进行高速余弦相似度检索，秒级返回 Top 100 候选。
  3.  **大模型重排 (Re-ranking)**：利用专业的 BGE Reranker 对初筛结果进行“像素级”的语义校对，剔除向量空间中距离近但逻辑不符的“假阳性”结果。

#### 1.1.1 动态相关性截断算法 (Dynamic Relevance Cutoff)

为避免返回大量低相关结果，系统采用**断崖检测 + 最低阈值**的双重过滤策略：

1.  **断崖检测**: 遍历按分数排序的结果，当相邻两个分数下降超过 20% 时，判定后续结果不相关，立即截断。
2.  **最低阈值**: 无论是否检测到断崖，分数低于 0.5 的结果一律过滤。

### 1.2 采集管道 (Ingestion Pipeline)：基于状态机的自动化演进

采集管道是系统的“心脏”，它在后台默默工作，负责图库的持续扩张与质量进化。

- **削峰填谷机制**：定时任务（Cron）发现新图后，并不会直接处理，而是将任务发送到 Cloudflare Queues。这样做的好处是，即使 Unsplash 瞬间发布了上百张图，后台的 Workflow 也会根据预设的并发上限（Concurrency Limit）匀速消耗，避免撑爆边缘 Worker 的内存或触发 AI 调用频控。
- **分布式状态机 (Workflows)**：每一张图片的处理都被封装为一个持久化的 Workflow 实例。无论是在下载图片、AI 分析还是向量入库的哪个环节发生网络抖动，Workflow 都会保留当前状态并在 30 秒后自动重试。这种设计保证了 D1、R2 和 Vectorize 之间的数据强一致性。

---

## 2. 核心算法专题

### 2.1 线性对撞采集模型 (Linear Boundary Ingestion)

针对 Unsplash API 免费版每小时仅 50 次请求的严苛限制，Lens 放弃了传统的全量扫描，开发了“线性对撞”算法。

**数学原理**：我们将图片库视为一个单向递增的时间轴。

1.  **正向追新 (Forward Phase)**：系统从 Page 1 开始向后探测。
    - **熔断判定 (`hitExisting`)**：对每一页数据，系统会立即与 D1 数据库进行 ID 比对。一旦在当前页发现任一 ID 已存在，说明系统已撞到了上次任务的“高水位线（High Water Mark）”。
    - **立即停止**：此时系统判定“新图区”已处理完毕，立即熔断正向抓取逻辑，将节省下来的 API 配额全部转入“历史回填”。
2.  **反向回填 (Backfill Phase)**：利用剩余配额，根据 D1 存储的游标记录，持续挖掘更早的历史图片，确保图库的深度。

### 2.2 自我进化循环 (Self-Evolution Logic)

这是一个基于 **“Neuron 余额捡漏”** 思想的异步升级机制。

- **策略**: 为了不干扰白天的正常采集和搜索，系统被设定为仅在 **UTC 23:00**（每日配额重置前的一小时）运行自进化。
- **动作**: 系统会计算今日累计消耗的 Neurons，如果剩余免费配额充足（扣除缓冲区后），会自动从 D1 挑选标记为 `llama-3.2` 的老图。
- **执行**: 直接从 R2 读取图片流（不消耗 API 配额），使用 **Llama 4 Scout** 进行重刷，补全旗舰级元数据。
- **收益**: 实现了全库数据在零额外支出前提下的自动平滑升级。

---

## 3. 全链路可观测性 (Observability)

Lens 引入了 **Trace-ID 链路追踪机制**，为分布式边缘架构提供了“上帝视角”的监控能力。

### 3.1 Trace-ID 生命周期

每一个业务脉冲（如搜索请求、Cron 任务、Workflow 实例）在进入系统时，都会被分配一个唯一的追踪指纹：

- **SEARCH-xxxx**: 用于追踪单次用户搜索的全生命周期。
- **CRON-xxxx**: 用于追踪每 10 分钟一次的抓取脉冲。
- **WF-photoId-xxxx**: 用于追踪特定图片从下载到入库的所有状态机步骤。

### 3.2 跨组件回溯

通过将 Trace-ID 贯穿于日志输出，运维人员可以在 Cloudflare 日志控制台中一键过滤出某一笔业务的所有日志，从而精准定位由于 AI 超时、D1 锁死或 R2 网络波动引发的偶发性故障。

---

## 4. AI 模型分层策略

系统针对不同场景采用了“旗舰模型重逻辑、专用模型重效率”的策略：

| 任务维度         | 选用模型            | 决策理由                                                                    |
| :--------------- | :------------------ | :-------------------------------------------------------------------------- |
| **深度视觉推理** | `Llama 4 Scout 17B` | 拥有 16 专家 MoE 架构，能识别出“这种构图传达了孤独感”级别的深层语义。       |
| **高维向量化**   | `BGE-M3 (1024d)`    | 工业级标准的多语言嵌入模型，对中文查询具备天然的亲和力。                    |
| **相关性精排**   | `BGE Reranker Base` | 专用判别式模型，其 Cross-attention 机制在判断“文图一致性”上远超生成式模型。 |
| **查询语义扩展** | `Llama 3.2 3B`      | 轻量快速的查询扩展模型，将用户的关键词扩展为丰富的视觉场景描述。            |

---

## 5. AI 资源分级调度模型 (Tiered Scheduling)

Lens 的核心竞争力不仅在于搜索，更在于其对边缘算力资源的**精细化套利**。我们根据业务价值将 AI 任务分为三个优先级：

### 5.1 实时交互级 (Interactive - P0)

- **场景**: 搜索请求（Query Expansion, Reranking）。
- **策略**: **延迟优先**。使用轻量级 3B 模型保证亚秒级响应，即使在高并发下也不牺牲用户体验。

### 5.2 增量时效级 (Incremental - P1)

- **场景**: 新图抓取。
- **策略**: **及时优先**。利用每 10 分钟一次的 Cron 脉冲，确保最新视觉资产在分钟级内完成 Llama 4 旗舰版分析。

### 5.3 余额自进化级 (Evolutionary - P2)

- **场景**: 存量数据升级。
- **策略**: **成本优先**。仅在 UTC 23:00 结算当日 Neurons 余额后，利用剩余的免费“残渣”额度执行重刷。

这种分级调度确保了系统在**“响应速度、时效性、经济性”**三者之间达到了完美的平衡。
